TODO 24/5:
    1. New learning rate curves, discuss why they are choppy and why we had to
        take such a low learning rate in order to get convergence:
        It is because (CHATGPT)
        """
        When the learning rate needs to be set at a very low value, such as 0.00007, to achieve a smooth convergence to a global minimum, it suggests a few things about the model and data:
        Steep Gradient Landscape: A low learning rate indicates that the model encounters steep or narrow regions in the optimization landscape. In such cases, larger learning rates can cause the model to overshoot the optimal solution, resulting in oscillations or instability. A low learning rate helps the model make smaller, more cautious steps towards convergence.
        Complex or Noisy Data: If the dataset is complex or noisy, it can lead to a more challenging optimization problem. Noisy data can introduce fluctuations in gradients, making it difficult for the model to find a stable solution. Additionally, complex data distributions or relationships may require the model to explore a more intricate optimization landscape, which demands a lower learning rate for smoother convergence.
        Large Model or High-Dimensional Data: Models with a large number of parameters or high-dimensional data often have more complex optimization landscapes. In such cases, a lower learning rate might be necessary to navigate the landscape effectively and find a global minimum. Higher learning rates could cause the model to skip over or get stuck in suboptimal regions.
        Insufficient Data: If the training dataset is relatively small, the model may struggle to generalize well or learn the underlying patterns. In such cases, a lower learning rate can help prevent overfitting and improve the model's ability to converge smoothly. With limited data, a high learning rate might cause the model to memorize the training examples instead of learning meaningful representations.
        Regularization Techniques: The use of regularization techniques like weight decay (L2 regularization) or dropout can impact the learning rate's optimal value. These techniques introduce additional constraints or noise during training, which may require a lower learning rate to achieve convergence.
        """
    2. Perform EDA (explanatory data analysis).
        Is monoz more noisy than monojet. This would leed to choppier curves which makes sense
        Make a plot of the distributions of masses
        Are there outliers?

    3. Analyse the performance of the regressor models based on which masses.
        Is it better on low masses and worse on high?
